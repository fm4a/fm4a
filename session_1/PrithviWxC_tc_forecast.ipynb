{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46e878bd-d151-46ee-a027-d95d93a74cf1",
   "metadata": {},
   "source": [
    "# Tropical Cyclone forecast\n",
    "\n",
    "This notebook demonstrates how to use the Prithvi-WxC model to perform global weather forecasts. The specific case we will be looking at is the evolution of Hurricane Ida August 27 to September 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "448e7b70-fff1-4dad-84f1-36a84fc6c688",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be56aa40-248d-4910-8c3a-20d443da9f86",
   "metadata": {},
   "source": [
    "## Input data\n",
    "\n",
    "We will initialize the forecast at 0Z on August 2021-08-27. To perform long-range forecasts with the Prithvi-WxC model, we need to unroll the model. That means we will perform multiple forecasts reusing the predicted state as input for the next forecast. For this we will need static input data as well as climatology data for all intermediate forecast steps. We can download all required data using the ``get_prithvi_wxc_input`` function.\n",
    "\n",
    "> **Note:** To download the data you will need your NASA EarthData account again. Please see the ``PrithviWxC_input_data.ipynb`` notebook for instruction on how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b145493a-5395-4791-bfb0-517d4e8b7f7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simon/miniconda3/envs/fm4a/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "GES DISC username:  pansat\n",
      "GES DISC password:  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading MERRA-2 files.:   0%|                                                | 0/41 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "401 Client Error: Unauthorized for url: https://urs.earthdata.nasa.gov/oauth/authorize/?scope=uid&app_type=401&client_id=e2WVk8Pw6weeLUKZYOxvTQ&response_type=code&redirect_uri=https%3A%2F%2Fgoldsmr5.gesdisc.eosdis.nasa.gov%2Fdata-redirect&state=aHR0cHM6Ly9nb2xkc21yNS5nZXNkaXNjLmVvc2Rpcy5uYXNhLmdvdi9kYXRhL01FUlJBMi9NMkkzTlZBU00uNS4xMi40LzIwMjEvMDgvTUVSUkEyXzQwMS5pbnN0M18zZF9hc21fTnYuMjAyMTA4MzAubmM0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPrithviWxC\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdownload\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_prithvi_wxc_input\n\u001b[32m      4\u001b[39m initialization_time =np.datetime64(\u001b[33m\"\u001b[39m\u001b[33m2021-08-27\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mget_prithvi_wxc_input\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitialization_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# The time difference in hours between consecutive model steps.\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m24\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Total maximum lead time.\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../data/merra-2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../data/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fm4a/lib/python3.12/site-packages/PrithviWxC/download.py:528\u001b[39m, in \u001b[36mget_prithvi_wxc_input\u001b[39m\u001b[34m(time, input_time_step, lead_time, input_data_dir, download_dir)\u001b[39m\n\u001b[32m    525\u001b[39m all_steps = \u001b[38;5;28mlist\u001b[39m(input_times) + \u001b[38;5;28mlist\u001b[39m(output_times)\n\u001b[32m    527\u001b[39m LOGGER.info(\u001b[33m\"\u001b[39m\u001b[33mDownloading MERRA-2 files.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m528\u001b[39m merra_files = \u001b[43mdownload_merra_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_dir\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mraw\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    530\u001b[39m days = [time.astype(\u001b[33m\"\u001b[39m\u001b[33mdatetime64[s]\u001b[39m\u001b[33m\"\u001b[39m).item() \u001b[38;5;28;01mfor\u001b[39;00m time \u001b[38;5;129;01min\u001b[39;00m all_steps]\n\u001b[32m    531\u001b[39m days = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m([datetime(year=day.year, month=day.month, day=day.day) \u001b[38;5;28;01mfor\u001b[39;00m day \u001b[38;5;129;01min\u001b[39;00m days]))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fm4a/lib/python3.12/site-packages/PrithviWxC/download.py:262\u001b[39m, in \u001b[36mdownload_merra_files\u001b[39m\u001b[34m(time_steps, destination)\u001b[39m\n\u001b[32m    260\u001b[39m             files.append(task.result())\n\u001b[32m    261\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m files\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fm4a/lib/python3.12/site-packages/PrithviWxC/download.py:260\u001b[39m, in \u001b[36mdownload_merra_files\u001b[39m\u001b[34m(time_steps, destination)\u001b[39m\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m tqdm(as_completed(tasks), total=\u001b[38;5;28mlen\u001b[39m(tasks), desc=\u001b[33m\"\u001b[39m\u001b[33mDownloading MERRA-2 files.\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    259\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m         files.append(\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    261\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    262\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fm4a/lib/python3.12/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fm4a/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fm4a/lib/python3.12/concurrent/futures/thread.py:59\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fm4a/lib/python3.12/site-packages/PrithviWxC/download.py:202\u001b[39m, in \u001b[36mdownload_merra_file\u001b[39m\u001b[34m(url, destination, force, credentials)\u001b[39m\n\u001b[32m    200\u001b[39m redirect = session.get(url, auth=auth)\n\u001b[32m    201\u001b[39m response = session.get(redirect.url, auth=auth, stream=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(destination, \u001b[33m\"\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m output:\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m response:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fm4a/lib/python3.12/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1021\u001b[39m     http_error_msg = (\n\u001b[32m   1022\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1023\u001b[39m     )\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 401 Client Error: Unauthorized for url: https://urs.earthdata.nasa.gov/oauth/authorize/?scope=uid&app_type=401&client_id=e2WVk8Pw6weeLUKZYOxvTQ&response_type=code&redirect_uri=https%3A%2F%2Fgoldsmr5.gesdisc.eosdis.nasa.gov%2Fdata-redirect&state=aHR0cHM6Ly9nb2xkc21yNS5nZXNkaXNjLmVvc2Rpcy5uYXNhLmdvdi9kYXRhL01FUlJBMi9NMkkzTlZBU00uNS4xMi40LzIwMjEvMDgvTUVSUkEyXzQwMS5pbnN0M18zZF9hc21fTnYuMjAyMTA4MzAubmM0"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PrithviWxC.download import get_prithvi_wxc_input\n",
    "\n",
    "initialization_time =np.datetime64(\"2021-08-27\")\n",
    "get_prithvi_wxc_input(\n",
    "    initialization_time,\n",
    "    6, # The time difference in hours between consecutive model steps.\n",
    "    6 * 24, # Total maximum lead time.\n",
    "    \"../data/merra-2\",\n",
    "    \"../data/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576ceb53-992a-4fe7-9d23-7b393b9b8282",
   "metadata": {},
   "source": [
    "## Loading the input data\n",
    "\n",
    "To load the input data for the roll-out forecast, we use the ``Merra2RolloutDataset``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2a3d94-e6fa-4008-8a6a-b3e2f0f8ebd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from PrithviWxC.configs import get_model_config\n",
    "from PrithviWxC.dataloaders.merra2_rollout import Merra2RolloutDataset\n",
    "\n",
    "# Workshop participants can use the 'small' Prithvi-WxC model\n",
    "# Remote users will need to use 'large_rollout'.\n",
    "config_name = \"small\"\n",
    "prithvi_config = get_model_config(config_name, data_dir=\"../../data\")\n",
    "\n",
    "dataset = Merra2RolloutDataset(\n",
    "    time_range=(\"2021-08-26\", \"2021-09-18\"),\n",
    "    lead_time=96,\n",
    "    input_time=-6,\n",
    "    data_path_surface=\"../data/merra-2\",\n",
    "    data_path_vertical=\"../data/merra-2\",\n",
    "    climatology_path_surface=\"../data/merra-2/climatology/climatology\",\n",
    "    climatology_path_vertical=\"../data/merra-2/climatology/climatology\",\n",
    "    surface_vars=prithvi_config.surface_vars,\n",
    "    static_surface_vars=prithvi_config.static_surface_vars,\n",
    "    vertical_vars=prithvi_config.vertical_vars,\n",
    "    levels=prithvi_config.levels,\n",
    "    positional_encoding=\"fourier\",\n",
    ")\n",
    "len(dataset) > 0, \"There doesn't seem to be any valid data.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719c3164-ba2a-478c-8f6d-ebaa211cd505",
   "metadata": {},
   "source": [
    "## Loading the model\n",
    "\n",
    "In the cell below we first create the model instance and then load the pre-trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea83771-88c4-45c6-8eb8-cff1ab3f47ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from PrithviWxC.configs import load_model\n",
    "\n",
    "config_name = \"small\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "if config_name == \"large\":\n",
    "    model = load_model(config_name, data_dir=\"../data\", load_weights=True)\n",
    "else:\n",
    "    model = load_model(config_name, data_dir=\"../data\", load_weights=False)\n",
    "    weights_path = \"../data/weights/prithvi.wxc.rollout.600m.v1.pt\"\n",
    "    state_dict = torch.load(weights_path, weights_only=False)\n",
    "    if \"model_state\" in state_dict:\n",
    "        state_dict = state_dict[\"model_state\"]\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "    \n",
    "if (hasattr(model, \"device\") and model.device != device) or not hasattr(\n",
    "    model, \"device\"\n",
    "):\n",
    "    model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1975574f-7a4e-458d-923f-0fb29a32a51f",
   "metadata": {},
   "source": [
    "## Running the forecast\n",
    "\n",
    "To perform forecasts that extend over several days with the Prithvi-WxC model, we need to unroll the model. That means the forecast is produced by repeatedly forcasting the next atmospheric state with a fixed, given lead time, and reusing the resulting forecasted state to forecast the next state. Since the ``Prithvi-WxC`` package provides a function that perform the unrolling for us, we don't need to worry to much about the details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950824af-7732-4be2-a784-d28c8cec613b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, nn\n",
    "\n",
    "\n",
    "def rollout_iter(\n",
    "    nsteps: int,\n",
    "    model: nn.Module,\n",
    "    batch: dict[str, Tensor | int | float],\n",
    "    return_intermediate: bool = False\n",
    ") -> Tensor:\n",
    "    \"\"\"A helper function for performing autoregressive rollout.\n",
    "\n",
    "    Args:\n",
    "        nsteps (int): The number of rollout steps to take\n",
    "        model (nn.Module): A model.\n",
    "        batch (dict): A data dictionary common to the Prithvi models.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the number of steps isn't positive.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: the output of the model after nsteps autoregressive iterations.\n",
    "    \"\"\"\n",
    "    if nsteps < 1:\n",
    "        raise ValueError(\"'nsteps' shouold be a positive int.\")\n",
    "\n",
    "    xlast = batch[\"x\"][:, 1]\n",
    "    batch[\"lead_time\"] = batch[\"lead_time\"][..., 0]\n",
    "\n",
    "    # Save the masking ratio to be restored later\n",
    "    mask_ratio_tmp = model.mask_ratio_inputs\n",
    "\n",
    "    preds = []\n",
    "\n",
    "    for step in range(nsteps):\n",
    "        # After first step, turn off masking\n",
    "        if step > 0:\n",
    "            model.mask_ratio_inputs = 0.0\n",
    "\n",
    "        batch[\"static\"] = batch[\"statics\"][:, step]\n",
    "        batch[\"climate\"] = batch[\"climates\"][:, step]\n",
    "        batch[\"y\"] = batch[\"ys\"][:, step]\n",
    "\n",
    "        out = model(batch)\n",
    "        if return_intermediate:\n",
    "            preds.append(out.cpu().numpy())\n",
    "\n",
    "        batch[\"x\"] = torch.cat((xlast[:, None], out[:, None]), dim=1)\n",
    "        xlast = out\n",
    "\n",
    "    # Restore the masking ratio\n",
    "    model.mask_ratio_inputs = mask_ratio_tmp\n",
    "\n",
    "    if return_intermediate:\n",
    "        return preds\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137d16bb-1b14-4bbd-af9f-2a2ee2243f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PrithviWxC.dataloaders.merra2_rollout import preproc\n",
    "\n",
    "padding = {\"level\": [0, 0], \"lat\": [0, -1], \"lon\": [0, 0]}\n",
    "data = next(iter(dataset))\n",
    "batch = preproc([data], padding)\n",
    "\n",
    "for k, v in batch.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        batch[k] = v.to(device)\n",
    "\n",
    "rng_state_1 = torch.get_rng_state()\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    forecast = rollout_iter(dataset.nsteps, model, batch, return_intermediate=True)\n",
    "valid_times = initialization_time + np.timedelta64(6, \"h\") * np.arange(1, len(forecast) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0d7401-9290-4aba-9c2e-c0ec1dd8a20e",
   "metadata": {},
   "source": [
    "## Display results\n",
    "\n",
    "Since we are primarily interested in Hurricane Ida's track, we extract a domain over the west Atlantic from the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af36d6cb-32c6-431a-b9f7-bc50b2664a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lats = np.rad2deg(batch[\"static\"][0, 0, :, 0].cpu().numpy())\n",
    "lons = np.rad2deg(batch[\"static\"][0, 1, 0, :].cpu().numpy())\n",
    "\n",
    "lon_min = -100\n",
    "lon_max = -70\n",
    "lat_min = 15\n",
    "lat_max = 40\n",
    "\n",
    "lat_mask = (lat_min < lats) * (lats < lat_max)\n",
    "lon_mask = (lon_min < lons) * (lons < lon_max)\n",
    "\n",
    "forecast = [tnsr[..., lat_mask, :][..., lon_mask] for tnsr in forecast]\n",
    "lats = lats[lat_mask]\n",
    "lons = lons[lon_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81433d88-8e16-4b50-a8b2-f7439704359b",
   "metadata": {},
   "source": [
    "Below we extract the Hurricane track from the forecasts by tracking the location of the sea-level-pressure minimum throughout the forecast. We also load Ida's actual track from the HURDAT 2 database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90e3867-3cea-4bb4-92dd-21d299f1af2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "slp_fcst = [tnsr[0, 9] for tnsr in forecast]\n",
    "track = []\n",
    "for slp in slp_fcst:\n",
    "    row_ind, col_ind = np.unravel_index(np.argmin(slp), slp.shape)\n",
    "    track.append([lons[col_ind], lats[row_ind]])\n",
    "\n",
    "track = np.array(track)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681df536-d80f-45af-9d0c-a04d0902b295",
   "metadata": {},
   "source": [
    "## Get HURDAT reference track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee360999-98b5-406e-9fe0-5d16ff723baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "\n",
    "def download_hurdat2() -> Path:\n",
    "    \"\"\"\n",
    "    Download HURDAT 2 database.\n",
    "\n",
    "    Return:\n",
    "        A path object pointing to the download data if it isn't already present locally.\n",
    "    \"\"\"\n",
    "    dest = Path() / \"hurdat2.txt\"\n",
    "    if not dest.exists():\n",
    "        url = \"https://www.nhc.noaa.gov/data/hurdat/hurdat2-1851-2024-040425.txt\"\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        with open(dest, \"wb\") as output:\n",
    "            output.write(response.content)\n",
    "    return dest\n",
    "\n",
    "\n",
    "def parse_single_storm_xarray(filename, storm_id):\n",
    "    storm_id = storm_id.strip().upper()\n",
    "\n",
    "    with open(filename, 'r') as f:\n",
    "        while True:\n",
    "            header = f.readline()\n",
    "            if not header:\n",
    "                raise ValueError(f\"Storm '{storm_id}' not found in file.\")\n",
    "\n",
    "            parts = [p.strip() for p in header.strip().split(',')]\n",
    "            s_id, name, n_lines = parts[0], parts[1], int(parts[2])\n",
    "\n",
    "            if s_id.upper() != storm_id:\n",
    "                # Skip this storm\n",
    "                for _ in range(n_lines):\n",
    "                    f.readline()\n",
    "                continue\n",
    "\n",
    "            # Matching storm found\n",
    "            times, record_ids, lats, lons, winds, pressures = [], [], [], [], [], []\n",
    "\n",
    "            for _ in range(n_lines):\n",
    "                line = f.readline().strip()\n",
    "                fields = [x.strip() for x in line.split(',')]\n",
    "\n",
    "                dt = pd.to_datetime(f\"{fields[0]} {fields[1]}\", format=\"%Y%m%d %H%M\")\n",
    "                lat = float(fields[4][:-1]) * (1 if fields[4][-1] == 'N' else -1)\n",
    "                lon = float(fields[5][:-1]) * (1 if fields[5][-1] == 'E' else -1)\n",
    "                wind = int(fields[6])\n",
    "                pressure = fields[7]\n",
    "                pressure = int(pressure) if pressure else np.nan\n",
    "                record_id = fields[2]\n",
    "\n",
    "                times.append(dt)\n",
    "                lats.append(lat)\n",
    "                lons.append(lon)\n",
    "                winds.append(wind)\n",
    "                pressures.append(pressure)\n",
    "                record_ids.append(record_id)\n",
    "\n",
    "            # Return as xarray\n",
    "            return xr.Dataset(\n",
    "                data_vars={\n",
    "                    \"lat\": (\"time\", lats),\n",
    "                    \"lon\": (\"time\", lons),\n",
    "                    \"wind\": (\"time\", winds),\n",
    "                    \"pressure\": (\"time\", pressures),\n",
    "                    \"record_id\": (\"time\", record_ids),\n",
    "                },\n",
    "                coords={\n",
    "                    \"time\": times,\n",
    "                    \"storm_id\": s_id,\n",
    "                    \"storm_name\": name,\n",
    "                }\n",
    "            )\n",
    "\n",
    "\n",
    "def get_hurdat_track(storm_id: str) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Get HURDAT track for a storm with a given storm_id.\n",
    "    \"\"\"\n",
    "    hurdat_file = download_hurdat2()\n",
    "    return parse_single_storm_xarray(hurdat_file, storm_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9eaf2e-1fd7-4780-a3a1-dc4ffed38322",
   "metadata": {},
   "outputs": [],
   "source": [
    "hurdat = get_hurdat_track(\"AL092021\")\n",
    "time_mask = (valid_times.min() <= hurdat.time) * (hurdat.time <= valid_times.max())\n",
    "hurdat = hurdat[{\"time\": time_mask}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ca1ea9-8066-4529-8c09-de90b86752e7",
   "metadata": {},
   "source": [
    "## Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2741395-b686-4704-b004-078e78246d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "\n",
    "crs = ccrs.PlateCarree()\n",
    "fig = plt.figure(figsize=(8, 5))\n",
    "\n",
    "step = 8\n",
    "valid_time = valid_times[step].astype(\"datetime64[s]\").item()\n",
    "\n",
    "ax = fig.add_subplot(1, 1, 1, projection=crs)\n",
    "m = ax.contourf(lons, lats, slp_fcst[step])\n",
    "ax.plot(track[:, 0], track[:, 1], c=\"red\", label=\"Prithvi-WxC\")\n",
    "ax.plot(hurdat.lon, hurdat.lat, ls=\"--\", color=\"grey\", label=\"HURDAT 2\")\n",
    "ax.set_title(valid_time.strftime(\"Prithvi-WxC forecast at %Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "ax.coastlines()\n",
    "ax.legend()\n",
    "\n",
    "plt.colorbar(m, label=\"SLP [Pa]\", ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4244c736-64e2-482b-a2bf-59b9dbb6e934",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
