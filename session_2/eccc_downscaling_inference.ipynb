{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4548c3cc-7537-4d14-8d3e-c703e08098df",
   "metadata": {},
   "source": [
    "# Prithvi WxC Downscaling with ECCC Data: Model Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6864a0a-e36c-4993-a274-db11a46f10f9",
   "metadata": {},
   "source": [
    "This notebook is a walkthrough on using a fine-tuned downscaling model for generating inferences\n",
    "\n",
    "We show how to initalize the model, load the `Prithvi` fine-tuned weights, and use the model for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416fed88",
   "metadata": {},
   "source": [
    "To replicate the results show in this notebook please download the required files from our [Hugging Face](https://huggingface.co/ibm-granite/granite-geospatial-wxc-downscaling) repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec0cf6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simon/miniconda3/envs/fm4a/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "import os\n",
    "\n",
    "if not os.path.exists('granite-geospatial-wxc-downscaling'):\n",
    "    snapshot_download(repo_id='ibm-granite/granite-geospatial-wxc-downscaling', allow_patterns=\"*\", repo_type='model', local_dir='./granite-geospatial-wxc-downscaling')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cddac3",
   "metadata": {},
   "source": [
    "**This notebook is a simple plug-and-play example** \n",
    "\n",
    "We provide only **1 data sample**. See `./examples/eccc_downscaling/notebooks/README.md` to download and preprocess the remaining files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edd1ff6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "Python >= 3.10 is required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4c13813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "logging.disable(logging.CRITICAL)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c24b003-2007-4bb2-9504-37050fcf16d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pysteps configuration file found at: /home/simon/miniconda3/envs/fm4a/lib/python3.12/site-packages/pysteps/pystepsrc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "\n",
    "from granitewxc.utils.config import get_config\n",
    "from granitewxc.utils.eccc_data import get_dataloaders_eccc\n",
    "from granitewxc.utils.plot import plot_eccc_results\n",
    "from granitewxc.models.model import get_finetune_model_UNET, get_finetune_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb863eb",
   "metadata": {},
   "source": [
    "Configure the backends, PyTorch states, and random seeds to standardize the RNG for random crops in this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd08bdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.jit.enable_onednn_fusion(True)\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.cuda.manual_seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca2bc17",
   "metadata": {},
   "source": [
    "It is possible to use a cpu or gpu/s to generate inferences. Based on avaiablity of a `cuda:gpu`, we set the device that the model uses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba72be6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741ffef9",
   "metadata": {},
   "source": [
    "## Configuration File\n",
    "\n",
    "The model is configured using `YAML` files.\n",
    "\n",
    "In these files, you specify:\n",
    "- Paths to the input data  \n",
    "- Locations of the pretrained weights  \n",
    "\n",
    "To ensure compatibility with the provided weights during inference, keep the model configuration consistent with the original definitions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41c26cb3-e81f-4081-9944-1c67cc58c8d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config_path = './granite-geospatial-wxc-downscaling/ECCC/configs/config_UNET.yaml'\n",
    "config = get_config(config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de2aed1",
   "metadata": {},
   "source": [
    "## Dataloader \n",
    "\n",
    "In this example we will use only **1 sample of data**\n",
    "\n",
    "To download and setup all the remaining data follow the instructions in `./examples/eccc_downscaling/notebooks/README.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d862b7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Test samples: 1\n"
     ]
    }
   ],
   "source": [
    "test_dl = get_dataloaders_eccc(config, test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65bdc29",
   "metadata": {},
   "source": [
    "## Model Initialization\n",
    "\n",
    "We provide **2** different model architectures `UNET-like` and `CONV` \n",
    "\n",
    "Both architectures include:  \n",
    "1. **Patch Embedding**: Extracts shallow features from the input data  \n",
    "2. **Feature Extraction**: Utilizes the Prithvi backbone to extract deeper features  \n",
    "\n",
    "The key difference is that the UNET-like version incorporates **static high-resolution data** into the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3f273b",
   "metadata": {},
   "source": [
    "In this notebook, we use the **UNET-like** version\n",
    "\n",
    "To switch to the **CONV** model, update the configuration file accordingly and use `get_finetune_model(config)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ceb7d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the model.\n",
      "--> model has 1,448,690,568 params.\n"
     ]
    }
   ],
   "source": [
    "model = get_finetune_model_UNET(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a118d7",
   "metadata": {},
   "source": [
    "We can now load the pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d544fd5c-1f80-4fee-92cf-5a8165b03432",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 47.50 GiB of which 49.12 MiB is free. Process 1094577 has 540.00 MiB memory in use. Process 1410371 has 540.00 MiB memory in use. Process 3152761 has 34.85 GiB memory in use. Including non-PyTorch memory, this process has 10.98 GiB memory in use. Of the allocated memory 10.55 GiB is allocated by PyTorch, and 13.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m weights_path = config.path_model_weights\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m weights = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      4\u001b[39m model.load_state_dict(weights, strict=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      5\u001b[39m model.to(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fm4a/lib/python3.12/site-packages/torch/serialization.py:1521\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1519\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[32m   1520\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1521\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1523\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1524\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_weights_only_unpickler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1525\u001b[39m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1526\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1527\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1528\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m pickle.UnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1529\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m pickle.UnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fm4a/lib/python3.12/site-packages/torch/serialization.py:2119\u001b[39m, in \u001b[36m_load\u001b[39m\u001b[34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[39m\n\u001b[32m   2117\u001b[39m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[32m   2118\u001b[39m _serialization_tls.map_location = map_location\n\u001b[32m-> \u001b[39m\u001b[32m2119\u001b[39m result = \u001b[43munpickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2120\u001b[39m _serialization_tls.map_location = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2122\u001b[39m torch._utils._validate_loaded_sparse_tensors()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fm4a/lib/python3.12/site-packages/torch/_weights_only_unpickler.py:532\u001b[39m, in \u001b[36mUnpickler.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    525\u001b[39m         \u001b[38;5;28mtype\u001b[39m(pid) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m\n\u001b[32m    526\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pid) > \u001b[32m0\u001b[39m\n\u001b[32m    527\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m torch.serialization._maybe_decode_ascii(pid[\u001b[32m0\u001b[39m]) != \u001b[33m\"\u001b[39m\u001b[33mstorage\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    528\u001b[39m     ):\n\u001b[32m    529\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m UnpicklingError(\n\u001b[32m    530\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOnly persistent_load of storage is allowed, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpid[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    531\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m532\u001b[39m     \u001b[38;5;28mself\u001b[39m.append(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpersistent_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m key[\u001b[32m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [BINGET[\u001b[32m0\u001b[39m], LONG_BINGET[\u001b[32m0\u001b[39m]]:\n\u001b[32m    534\u001b[39m     idx = (read(\u001b[32m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m key[\u001b[32m0\u001b[39m] == BINGET[\u001b[32m0\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m unpack(\u001b[33m\"\u001b[39m\u001b[33m<I\u001b[39m\u001b[33m\"\u001b[39m, read(\u001b[32m4\u001b[39m)))[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fm4a/lib/python3.12/site-packages/torch/serialization.py:2083\u001b[39m, in \u001b[36m_load.<locals>.persistent_load\u001b[39m\u001b[34m(saved_id)\u001b[39m\n\u001b[32m   2081\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2082\u001b[39m     nbytes = numel * torch._utils._element_size(dtype)\n\u001b[32m-> \u001b[39m\u001b[32m2083\u001b[39m     typed_storage = \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2084\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2085\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2087\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fm4a/lib/python3.12/site-packages/torch/serialization.py:2049\u001b[39m, in \u001b[36m_load.<locals>.load_tensor\u001b[39m\u001b[34m(dtype, numel, key, location)\u001b[39m\n\u001b[32m   2045\u001b[39m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[32m   2046\u001b[39m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[32m   2048\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch._guards.detect_fake_mode(\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2049\u001b[39m     wrap_storage = \u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2050\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2051\u001b[39m     storage._fake_device = location\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fm4a/lib/python3.12/site-packages/torch/serialization.py:1864\u001b[39m, in \u001b[36m_get_restore_location.<locals>.restore_location\u001b[39m\u001b[34m(storage, location)\u001b[39m\n\u001b[32m   1863\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrestore_location\u001b[39m(storage, location):\n\u001b[32m-> \u001b[39m\u001b[32m1864\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdefault_restore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fm4a/lib/python3.12/site-packages/torch/serialization.py:698\u001b[39m, in \u001b[36mdefault_restore_location\u001b[39m\u001b[34m(storage, location)\u001b[39m\n\u001b[32m    678\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    679\u001b[39m \u001b[33;03mRestores `storage` using a deserializer function registered for the `location`.\u001b[39;00m\n\u001b[32m    680\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    695\u001b[39m \u001b[33;03m       all matching ones return `None`.\u001b[39;00m\n\u001b[32m    696\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    697\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[32m--> \u001b[39m\u001b[32m698\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    699\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    700\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fm4a/lib/python3.12/site-packages/torch/serialization.py:637\u001b[39m, in \u001b[36m_deserialize\u001b[39m\u001b[34m(backend_name, obj, location)\u001b[39m\n\u001b[32m    635\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m location.startswith(backend_name):\n\u001b[32m    636\u001b[39m     device = _validate_device(location, backend_name)\n\u001b[32m--> \u001b[39m\u001b[32m637\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fm4a/lib/python3.12/site-packages/torch/storage.py:291\u001b[39m, in \u001b[36m_StorageBase.to\u001b[39m\u001b[34m(self, device, non_blocking)\u001b[39m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, torch.device):\n\u001b[32m    290\u001b[39m     device = torch.device(device)\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_to\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fm4a/lib/python3.12/site-packages/torch/_utils.py:101\u001b[39m, in \u001b[36m_to\u001b[39m\u001b[34m(self, device, non_blocking)\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     98\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_sparse, (\n\u001b[32m     99\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33msparse storage is not supported for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice.type.upper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m tensors\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    100\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     untyped_storage = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m     untyped_storage.copy_(\u001b[38;5;28mself\u001b[39m, non_blocking)\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m untyped_storage\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 47.50 GiB of which 49.12 MiB is free. Process 1094577 has 540.00 MiB memory in use. Process 1410371 has 540.00 MiB memory in use. Process 3152761 has 34.85 GiB memory in use. Including non-PyTorch memory, this process has 10.98 GiB memory in use. Of the allocated memory 10.55 GiB is allocated by PyTorch, and 13.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "weights_path = config.path_model_weights\n",
    "weights = torch.load(weights_path, map_location=device)['model']\n",
    "\n",
    "model.load_state_dict(weights, strict=True)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5834fb-31df-4732-a926-bb1376b9ad42",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Inference\n",
    "\n",
    "The model is now ready for inference. We are running an inference for one batch (in this example batch_size=1)\n",
    "\n",
    "Unlike training, where we used fixed-size random crops, for inference we will use the entire Canadian region, a (1280, 2528) image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0d3e93-736a-4209-b34c-f32ae1b32eaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "\n",
    "    batch = next(iter(test_dl))\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "    out = model(batch)\n",
    "\n",
    "    inputs = batch['x']\n",
    "    targets = batch['y']\n",
    "    outputs = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fd6c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.shape, targets.shape, outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e0a99e-3014-4f0d-b10d-b6f90572d5ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Plotting\n",
    "\n",
    "We set the variable names and extract the sample information for generating plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5ac91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_name = \"UUWE\" # UUWE or VVSN\n",
    "var_unit = \"m/s\"\n",
    "\n",
    "output_vars = [*config.data.output_vars]\n",
    "input_vars = [*config.data.input_surface_vars,\n",
    "              *product(config.data.vertical_pres_vars, config.data.input_level_pres),\n",
    "              *product(config.data.vertical_level1_vars, config.data.input_level1),\n",
    "              *product(config.data.vertical_level2_vars, config.data.input_level2),\n",
    "              *config.data.other\n",
    "             ]\n",
    "\n",
    "coarsening_factor = targets.shape[-1] / inputs.shape[-1]\n",
    "\n",
    "f\"Downscaling '{var_name}' at  by {coarsening_factor}x\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c444bf8b-e96c-4aca-89e5-eeaaaa5064bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(0, len(outputs)):\n",
    "    var = output_vars.index(var_name)\n",
    "    input_var = input_vars.index(var_name)\n",
    "    var_name_tile = var_name\n",
    "\n",
    "    plot_input = inputs[idx, input_var, :, :].cpu().numpy()\n",
    "    plot_target = targets[idx, var, : ,:].cpu().numpy()\n",
    "    plot_pred = outputs[idx, var, :, :].cpu().numpy()\n",
    "    plot_residual = plot_target - plot_pred\n",
    "\n",
    "    plot_eccc_results(plot_input, plot_pred, plot_target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
